"""Run a simple Gradio app with model from config file"""
import sys
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
from dotenv import load_dotenv
load_dotenv()

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python gradio_evaluate.py 'base_model_id' 'adapter_id' ")
        sys.exit(1)

    else:
        base_model_id = sys.argv[1]
        adapter_id = sys.argv[2]
        print(f"Running with base_model_id: {base_model_id} and adapter_id: {adapter_id}")

    # Load the base model and tokenizer from Hugging Face
    print("Loading base model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_id)
    tokenizer.pad_token_id = tokenizer.eos_token_id
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        device_map="auto",
        quantization_config=bnb_config,
        torch_dtype=torch.float16
    )
    
    # Load the qLoRA adapter using the PEFT library
    try:
        from peft import PeftModel
        model_ft = PeftModel.from_pretrained(model, adapter_id)
        model_ft.eval()
        print("QLoRA adapter loaded successfully.")
    except ImportError:
        print("peft library is not installed. Please install it to use the qLoRA adapter.")
        sys.exit(1)
    
    # Define a prediction function that uses the tokenizer's formatting template.
    # Here we assume a simple format: we prepend "User:" and append "Assistant:" to the prompt.
    def predict(prompt):
        # Use a simple prompt template; customize this if your tokenizer/model expects a different format.
        formatted_prompt = tokenizer.apply_chat_template([{"role": "user", "content": prompt}], add_generation_prompt=True, tokenize=False)
        input = tokenizer(formatted_prompt, return_tensors="pt", padding=True)
        input_ids = input["input_ids"].to("cuda")
        # Generate a response from the model
        output_ids = model_ft.generate(
            input_ids,
            max_length=2048,
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.eos_token_id,
            attention_mask=input["attention_mask"].to("cuda") #attention_mask was beeing loaded on CPU
        )
        output = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
        # Extract the generated assistant response (everything after "Assistant:")
        response = output.split(formatted_prompt)[-1].strip()
        return response

    # Create and launch the Gradio interface
    iface = gr.Interface(
        fn=predict,
        inputs=gr.Textbox(lines=2, placeholder="Enter your prompt here..."),
        outputs="text",
        title="QLoRA Chat Model",
        description="Enter your prompt and receive a response generated by the model with qLoRA adapter."
    )

    iface.launch()
